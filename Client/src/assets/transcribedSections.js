export default {
  "0g09DWYCGAU": [
    {
      name: "Title",
      text:
        "in this work we present an automatic algorithm to correct camera perspective distortion on portrait photos ",
      start: 0,
      stop: 6,
      source: "Subtitles",
      words: 16,
      sentences: 1.0666666666666667,
    },
    {
      name: "Introduction",
      text:
        "wide-angle cameras are getting more and more popular they allow photographers to capture more scene contents and fill with more people when taking group photos and selfies however perspective distortion is inevitable it makes faces near edges and corners look stretched squished and skewed to address this issue we introduce a new algorithm to restore undistorted faces without affecting other parts of the photo here's an example of our result after our correction faces look more natural and realistic ",
      start: 6,
      stop: 37,
      source: "Subtitles",
      words: 79,
      sentences: 5.266666666666667,
    },
    {
      name: "Results",
      text: "example of our result after our correction faces look more natural and realistic ",
      start: 31,
      stop: 37,
      source: "Subtitles",
      words: 13,
      sentences: 0.8666666666666667,
    },
    {
      name: "RelatedWork",
      text:
        "existing methods use global projections to reduce perspective distortion however they introduce artifacts that bend the straight edges in the scene our approach is based on the following observation while the perspective projection makes background look good the faces near corners are often squished on the other hand the stereographic projection makes faces look natural but distort straight lines therefore our key idea is to locally ",
      start: 38,
      stop: 63,
      source: "Subtitles",
      words: 66,
      sentences: 4.4,
    },
    {
      name: "Method",
      text:
        "apply the stereographic projection to face regions and apply the perspective projection to the background given an input image in the detected subject mask we compute a mesh that adapts to the stereographic projection on face regions while keeping the background unchanged after optimizing the mesh we warp the input image to obtain the corrected image the proposed algorithm is fully automatic and operates at an interactive raid on the mobile platform please refer to our paper for more details ",
      start: 63,
      stop: 94,
      source: "Subtitles",
      words: 80,
      sentences: 5.333333333333333,
    },
    {
      name: "Results",
      text:
        "finally we show visual comparisons between the input and output of our algorithm we test the proposed algorithm for wide-angle portraits taken from various scenarios including single person selfie and group photos indoor and outdoor scenes and different lighting conditions you when backgrounds contain human-made objects such as buildings windows doors and furnitures our method preserves their geometries without introducing artifacts you our method also performs well on challenging cases such as side faces in substantial occlusions by sunglasses a hat or arms you ",
      start: 94,
      stop: 161,
      source: "Subtitles",
      words: 84,
      sentences: 5.6,
    },
    { name: "EndingSlide", text: "", start: 161, stop: 165, source: "Subtitles", words: 0, sentences: 0.0 },
  ],
  "0ybLCfVeFL4": [
    {
      name: "Title",
      text: "in this walk we present the first ",
      start: 0,
      stop: 2,
      source: "Subtitles",
      words: 7,
      sentences: 0.4666666666666667,
    },
    {
      name: "Introduction",
      text:
        "text-based video editing approach that lets editors insert new text in addition to cutting copying and pasting the existing transcript text our poach allows editing at any point and synthesizes the corresponding correct lip synced video okay the market closed today with Apple's stock price at one hundred and ninety one point four five dollars per share here we replace ninety one point four with eighty two point two okay the market closed today with Apple's stock price at one hundred and eighty two point two five dollars per share ",
      start: 2,
      stop: 45,
      source: "Subtitles",
      words: 90,
      sentences: 6.0,
    },
    {
      name: "Method",
      text:
        "give in a video of a talking head and a transcript of the speech we first align the transcript to the video at the level of phonemes we also register a 3d parametric face model with the video given any edit operation we perform a Visine search to find the best match between sub sequences of phonemes in the Edit and the input video this step gives us parameters of our face model which we further blend to obtain temporally coherent edits we use these to synthesize a lower face region and combine it with the input to create a composite to bridge the domain gap between the composites and rail video footage we use a neural phase rendering approach this gives us the final edited video we now show more text-based editing ",
      start: 45,
      stop: 102,
      source: "Subtitles",
      words: 133,
      sentences: 8.866666666666667,
    },
    {
      name: "Demo",
      text:
        "results we can add new words in a sentence he will replace well for our Dow with why are you and here's another edit I love the smell of napalm in the morning I love the smell of french toast in the morning note that the synthesized words were not spoken by the subjects in the training video the audio of the new words were separately recorded we also provide examples using a synthesized voice I got hacen aided by your networks here we showed the retrieved frames which we'll use to synthesize the mouth motion I got a sedated vibe in your networks even though these frames come from different parts of the video in a not temporally coherent our method produces temporally smooth outputs I got a sedated vibe in your networks we can also synthesize audio using text-to-speech systems like the vocal system of genital in this example the audio for ice creams was generated using vocal she sells ice creams by the seashore we can also delete words learning from examples and and scientist over the last few decades learning from examples and scientist over the last few decades our yn are worried over silly items why n over silly items our results can be seamlessly composited into the original video sequence which allows us to edit videos of arbitrary resolutions she sells ice cream by the seashore in addition to such edits we can also synthesize full sentences just from text to give a virtual assistant a face she sells ice cream by the seashore here we again show the frames where the face model parameters were retrieved from she sells ice cream by the seashore she sells ice cream by the seashore did you hear about the crook who stole a calendar no I did not full sentence synthesis enables our method to be used for video translation here we enable our non German speaking subject to speak German each player dodge morph cut is a video transition tool in premiere poll ",
      start: 102,
      stop: 275,
      source: "Subtitles",
      words: 338,
      sentences: 22.533333333333335,
    },
    {
      name: "Evaluation",
      text:
        "for removing jump cuts that is based on the work of Berta soy Dahl in our setting morph cut produces artifacts as it requires the transition to be in a relatively static part of the video so deep learning is inside machine learning is one of the approaches to machine learning so deep learning is inside machine learning is one of the approaches to machine learning learning from example and scientist over the last few decades after about learning from examples and scientists over the last few decades after demo morph God cannot be used to composite short segments and thus cannot be used to blend our retrieved video sequences the market closed today with Apple stock price at one hundred and eighty two point two five dollars per share okay the market closed today with Apple stock price at one hundred and eighty two point two five dollars per share we compel on your face rendering network two deep video portraits of key metal as this apology does not perform text-based editing with / in a self reenactment setting where a test sequence is reproduced note for fairness we train their approach using our recurrent generator network the video portraits cannot deal with dynamic background or even dynamic foreground such as the motion of the hands or the shirt our method handles these challenging scenarios well we also synthesize a higher-quality mouth region compared to key metal pochi synthesizes a higher quality and temporally more stable mouth region than face to face of digital without our parameter blending strategy results are temporally incoherent the market closed today with Apple stock price at 180 2.25 dollars per share we blend the parameters of the face model in every transition region this leads to realistic looking motion which is well aligned to the edited text the market closed today with Apple stock price at 180 2.25 dollars per share I got into neural networks I got into neural networks here we evaluate result quality with respect to the size of the data set I love the smell of french toast in the morning I love the smell of french toast in the morning I love the smell of french toast in the morning I love the smell of french toast in the morning best results are obtained using the full data set the quality of results degrades gracefully with the size of the data set we evaluate the realism of our results by performing a web-based user study we asked users to rate the realism of source videos which we want to edit the dingo ate your baby really reference videos were the subjects speak the edited sentences the dingo ate my baby and edited videos obtained using our approach the dingo ate my baby she sells seashells by the seashore she sells ice cream by the seashore she sells ice cream by the seashore our results were considered realistic by more than half of the participants ",
      start: 275,
      stop: 521,
      source: "Subtitles",
      words: 496,
      sentences: 33.06666666666667,
    },
    {
      name: "EndingSlide",
      text: "thank you for watching ",
      start: 521,
      stop: 530,
      source: "Subtitles",
      words: 4,
      sentences: 0.26666666666666666,
    },
  ],
  "1hjHDwjzWwQ": [
    { name: "Title", text: "", start: 0, stop: 9, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Demo", text: "", start: 9, stop: 157, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Evaluation", text: "", start: 157, stop: 292, source: "SpeechToText", words: 0, sentences: 0.0 },
  ],
  "1kXd2099GfE": [
    { name: "Title", text: "", start: 0, stop: 3, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Method", text: "", start: 3, stop: 75, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Demo", text: "", start: 3, stop: 17, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Evaluation", text: "", start: 75, stop: 90, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Demo", text: "", start: 90, stop: 113, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Results", text: "", start: 90, stop: 113, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "EndingSlide", text: "", start: 113, stop: 120, source: "SpeechToText", words: 0, sentences: 0.0 },
  ],
  "7c6oQP1u2eQ": [
    {
      name: "Introduction",
      text:
        "animating characters is a difficult task when it comes to interacting with objects and the environment in this research represent the nearest state machine a data-driven deep learning framework for characters scene interactions the difficulty in such animations is that they require complex planning of periodic as well as a periodic movements to complete a given task creating them in a production ready quality is not straightforward and often very time-consuming instead our system can synthesize different movements and scene interactions from motion capture data and allows the user to seamlessly control the character and real-time from simple control commands since our model directly learns from a geometry the motions can naturally adapt to variations in the scene be sure that our system can generate a large variety of movements including locomotion sitting on chairs carrying boxes opening doors and avoiding obstacles all from a single model the neural state machine consists ",
      start: 0,
      stop: 53,
      source: "Subtitles",
      words: 152,
      sentences: 10.133333333333333,
    },
    {
      name: "Title",
      text:
        "animating characters is a difficult task when it comes to interacting with objects and the environment in this research represent the nearest state machine a data-driven deep learning framework for characters scene interactions the difficulty in such ",
      start: 0,
      stop: 14,
      source: "Subtitles",
      words: 37,
      sentences: 2.466666666666667,
    },
    {
      name: "Reflection",
      text:
        "can synthesize different movements and scene interactions from motion capture data and allows the user to seamlessly control the character and real-time from simple control commands since our model directly learns from a geometry the motions can naturally adapt to variations in the scene be sure that our system can generate a large variety of movements including locomotion sitting on chairs carrying boxes opening doors and avoiding obstacles all from a single model the neural state machine consists ",
      start: 26,
      stop: 53,
      source: "Subtitles",
      words: 78,
      sentences: 5.2,
    },
    {
      name: "SystemArchitecture",
      text:
        "of two main components the gating Network and the motion prediction network our model learns a set of expert rates and how to dynamically blend them in the gating Network which then composes the motion prediction network that animates the character using the face combined with the character state the system learns a multimodal segmentation of movements actions and transitions that are possible at each state he further use separate encoders for the character post scene geometry and user control signals to predict the character motion from one frame into the next all network components are trained together in an end-to-end fashion we ",
      start: 53,
      stop: 88,
      source: "Subtitles",
      words: 103,
      sentences: 6.866666666666666,
    },
    {
      name: "Method",
      text:
        "captured motions of walking and running in different directions as well as during set insurance and side steps we then set up objects of different geometry and captured very seen interactions and transitions from different starting locations our complete dataset contains two hours of motion capture data in order to make our system adapts to variations in the shape and type of such objects we developed an augmentation framework that enriches the space of possible movements for each interaction task first we once with an object accurately into a scene and compute the contact information we then extract the original motion curves for a set of key joints with respect to the contact points after retransform in or switching an object we map the contact points to the new object and recompute the original motion curves to satisfy the new contact States the recomputed motion curves are finally given to a full body in this kinematic solver for synthesizing the whole sequence of motion at every frame we can then randomly resample the shape and size of an object and compute the corresponding pose this technique augments the motion without increasing the data size and can be applied in the same fashion to all interaction tasks in our data during runtime the user can interact if Li navigate the character from low-level control signals the purpose fears show the go transformations given by the user from which the network generates suitable paths that the character will follow the same framework can also be used to control the character in a more high-level fashion this is helpful when navigating characters manually is rather difficult such as for accurately approaching and sitting on a chair given the object's location and the desired action the network automatically creates movements paths and transitions in order to learn from the geometry he first walks allies each object mesh and then introduced an interaction and environment sensor to extract the relevant shape and scene information the environment sensor is mainly responsible for capturing the course details around the character and the interaction sensor provides a higher resolution for a particular object since the motion is inferred from a geometry and control signals multiple animations can be generated on the same object by simply changing the desired action and without manually assigning labor's for each object for dynamic tasks the interaction sensor can also be used to define the target location for example we had to place down an object after picking it up without the environment censor the character can easily pass through objects on its path without the interaction sensor the final details of an object such as amorous might not be captured well enough to reach a given goal accurately we introduced a bi-directional control scheme that learns and matches the motion in both egocentric and goals entry coordinates without such technique the character is likely to miss or overshoot the desired goal location which results in misalignment or missing contacts between body and object we now show the learned face transitions ",
      start: 88,
      stop: 270,
      source: "Subtitles",
      words: 508,
      sentences: 33.86666666666667,
    },
    {
      name: "Results",
      text:
        "of the neural state machine for different animations on the top left each black circle represents one set of expert weights and the curve in the middle shows the interpolation between them for each type of motion the gating Network produces different activation patterns among the experts which indicates how well the motions are segmented such framework is scalable since it avoids undesired interpolations between different movements and only allows certain actions and transitions to appear depending on the current character state our framework uses the Kronecker product to combine the face and actions as input for the gating network if simply concatenating features given to the network the importance of temporal face alignments can easily get lost which can be observed from the blending coefficients at the bottom the results are significant foot sliding artifacts stiff motion as well as unresponsive control we now demonstrate our model being able to adapt motions of the same action to different types of objects for example sitting on chairs with or without arm rests or leaning against the desk our network can also learn the path and transitions to avoid obstacles for example when sitting on a chair behind a desk since the model has learned to naturally synthesize different movements a current action can be aborted at any time if the user changes the control signal the user can also interactive the size of an object and the motion smoothly adjusts the same also applies to certain changes in the location of an object in which case the network automatically recovers movements trajectories and actions when presenting novel or unseen objects to the network the generated animation still appear plausible and provide smooth transitions between them we now compare our model to different ",
      start: 270,
      stop: 381,
      source: "Subtitles",
      words: 292,
      sentences: 19.466666666666665,
    },
    {
      name: "Evaluation",
      text:
        "controllers when training a face function neural network on the full data set it creates strongly vibrating movements both here and locomotion and also during standing such artefacts become even more visible for interaction tasks for stable contacts are desired our model can synthesize such movements in a much higher quality since it learns suitable segmentations for each motion in the expert space the motor depth of neural network does not create such vibrating movements but instead shows difficulty producing sharp locomotion for biped characters when using the foot velocities as input to the gating Network LST m/s are another potential solution / tend to be less responsive than our framework and often require considerably more data for training if certain geometry or transitions between actions are not captured the animation might remain in its current state and fail to execute the user given control commands our framework provides a more reliable interpolation for unseen data in summary represent an end-to-end ",
      start: 381,
      stop: 441,
      source: "Subtitles",
      words: 161,
      sentences: 10.733333333333333,
    },
    {
      name: "Reflection",
      text:
        "deep learning framework for synthesizing character scene interactions for motion capture data called nura state machine our system can be used through simple user control commands and generates high quality motions that can adapt to different objects and environments the model is responsive compact and scalable and is the first of such frameworks to handle scene interaction tasks for data-driven character animation ",
      start: 441,
      stop: 468,
      source: "Subtitles",
      words: 62,
      sentences: 4.133333333333334,
    },
  ],
  "7l_Sytw79tw": [
    { name: "Title", text: "[Music] ", start: 0, stop: 4, source: "Subtitles", words: 0, sentences: 0.0 },
    { name: "Introduction", text: "", start: 4, stop: 16, source: "Subtitles", words: 0, sentences: 0.0 },
    { name: "SystemArchitecture", text: "", start: 16, stop: 32, source: "Subtitles", words: 0, sentences: 0.0 },
    { name: "Introduction", text: "", start: 32, stop: 42, source: "Subtitles", words: 0, sentences: 0.0 },
    { name: "SystemArchitecture", text: "", start: 42, stop: 48, source: "Subtitles", words: 0, sentences: 0.0 },
    {
      name: "Demo",
      text:
        "geon's and the relationship to curvy our tracking technologies so opportunity been used in decades for in computer science and in particular individual graphics as a good representation of orientations of 3d objects because you can have their number of different ways you can represent orientations but in particular or turn hands give you a lot of super nice conveniences that make sense moving from one orientation to another very su and we describe orientations in 3d space but they're actually representative of rotations in 4-dimensional space and so you can have you can have one return you represent a sort of what's called a double rotation in four-dimensional space where you generally I really enjoyed it and then ",
      start: 48,
      stop: 96,
      source: "Subtitles",
      words: 119,
      sentences: 7.933333333333334,
    },
    {
      name: "Evaluation",
      text:
        "surprisingly it was very relevant that's not really like this so in the first session I am I on purpose decided to look the way immediately it was like scary how that crater was well she I didn't realize those goes and not win this thing reminded me I was like oh yeah a wood stove ",
      start: 96,
      stop: 118,
      source: "Subtitles",
      words: 56,
      sentences: 3.7333333333333334,
    },
    {
      name: "Demo",
      text: "[Music] you [Music] [Music] ",
      start: 118,
      stop: 200,
      source: "Subtitles",
      words: 1,
      sentences: 0.06666666666666667,
    },
    { name: "EndingSlide", text: "", start: 200, stop: 210, source: "Subtitles", words: 0, sentences: 0.0 },
  ],
  "7suNQpNGDE8": [
    { name: "Title", text: "", start: 0, stop: 12, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Introduction", text: "", start: 0, stop: 80, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Demo", text: "", start: 80, stop: 165, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Title", text: "", start: 166, stop: 186, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "EndingSlide", text: "", start: 186, stop: 196, source: "SpeechToText", words: 0, sentences: 0.0 },
  ],
  "8gDV6tPNXQs": [
    {
      name: "Title",
      text: "a virtual try own system for prescription eyeglasses we propose a ",
      start: 0,
      stop: 5,
      source: "Subtitles",
      words: 11,
      sentences: 0.7333333333333333,
    },
    {
      name: "Introduction",
      text:
        "prescription eyeglasses trial system which can build corrective lens model based on the user's prescription and generate a synthesized video allowing a user to virtually try on a variety of prescription eyeglasses our realistic simulation of refraction effect increases the perceived realism according to the user study virtual try ",
      start: 5,
      stop: 33,
      source: "Subtitles",
      words: 49,
      sentences: 3.2666666666666666,
    },
    {
      name: "SystemArchitecture",
      text:
        "own pipeline the proposed approach takes as input the user's image sequence eyeglasses prescription and a frame model the pipeline consists of two stages virtual eyeglasses generation and video synthesis inspired by the traditional eyeglasses manufacturing process the virtual eyeglasses are generated with three steps I classes positioning creation for parametric lens model and Lance cutting and mounting in the video synthesis stage we render the virtual eyeglasses and insert them into the input image sequence taking into account impacts of refraction reflection and shadows the output is a synthesis to video where the user is wearing the virtual prescription eyeglasses we will now describe each step in detail we first position the eyeglasses onto ",
      start: 33,
      stop: 86,
      source: "Subtitles",
      words: 115,
      sentences: 7.666666666666667,
    },
    {
      name: "Method",
      text:
        "the users face geometry manual positioning of the eyeglasses is down for the first frame for the subsequent frames we use face tracking for automatic positioning we then create a parametric lens model based on the user's prescription lenses are aligned with the users eye center based on the optic axis we trim the lenses and mount them into the eyeglasses frame the virtual eyeglasses are then rendered and inserted into the input image sequence taking into account I glasses frame lenses and the surrounding 90 virtual try own results given the input image ",
      start: 86,
      stop: 127,
      source: "Subtitles",
      words: 94,
      sentences: 6.266666666666667,
    },
    {
      name: "Results",
      text:
        "sequence we insert eyeglasses frame and refraction and reflection effects the user can choose different eyeglasses frames and lens materials thanks to the refraction effect our virtual trail video looks quite similar to the real reference video the video is playing at 40% of the original speed for clear view ",
      start: 127,
      stop: 157,
      source: "Subtitles",
      words: 50,
      sentences: 3.3333333333333335,
    },
    {
      name: "Evaluation",
      text:
        "use the study we performed a user study to assess the perceived realism of virtual travel videos and study the effects of refraction and reflection five videos were displayed including four variations of our method with refraction with reflection with refraction without reflection without refraction with reflection without refraction without reflection and one recorded video from online a glasses store in each trial the subject was asked to rent those videos by dragging them over the screen into ranking beings 20 individuals participated in our study each of them completed 14 trials the overall opinion favorite videos which exhibit both refraction and reflections more results here we show synthesized ",
      start: 157,
      stop: 209,
      source: "Subtitles",
      words: 109,
      sentences: 7.266666666666667,
    },
    {
      name: "Results",
      text:
        "videos of users were in different eye classes with various Lance prescription and properties this is our virtual try ",
      start: 209,
      stop: 224,
      source: "Subtitles",
      words: 19,
      sentences: 1.2666666666666666,
    },
    {
      name: "EndingSlide",
      text: "own systems for prescription eyeglasses thank you for watching ",
      start: 224,
      stop: 232,
      source: "Subtitles",
      words: 9,
      sentences: 0.6,
    },
  ],
  "8nMXiUyq3tg": [
    { name: "Title", text: "", start: 0, stop: 7, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Introduction", text: "", start: 7, stop: 14, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Method", text: "", start: 14, stop: 44, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "SystemArchitecture", text: "", start: 14, stop: 44, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Demo", text: "", start: 44, stop: 52, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Method", text: "", start: 52, stop: 59, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "SystemArchitecture", text: "", start: 52, stop: 59, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Demo", text: "", start: 59, stop: 82, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Method", text: "", start: 82, stop: 94, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "SystemArchitecture", text: "", start: 82, stop: 94, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Demo", text: "", start: 94, stop: 120, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "EndingSlide", text: "", start: 120, stop: 133, source: "SpeechToText", words: 0, sentences: 0.0 },
  ],
  "8xjJ60W3_Fo": [
    { name: "Demo", text: "", start: 0, stop: 7, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Title", text: "", start: 7, stop: 10, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Method", text: "", start: 10, stop: 77, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Results", text: "", start: 77, stop: 173, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "EndingSlide", text: "", start: 173, stop: 176, source: "SpeechToText", words: 0, sentences: 0.0 },
  ],
  CmSIAoa8vqE: [
    {
      name: "Title",
      text: "we present digits a set of mechanical ",
      start: 0,
      stop: 2,
      source: "Subtitles",
      words: 7,
      sentences: 0.4666666666666667,
    },
    {
      name: "Introduction",
      text:
        "modules that augment mobile device interaction with tangible user interfaces the design of digits is based ",
      start: 2,
      stop: 13,
      source: "Subtitles",
      words: 16,
      sentences: 1.0666666666666667,
    },
    {
      name: "Method",
      text:
        "on a key observation that when the user presses a tactile button the finger force will cause a small shift of the device although this shift is subtle and transient it can be detected by the accelerometer also known as the inertial measurement unit equipped on the mobile device here's how this happens when we hold a phone still the finger force is applied on the phone balance each other while the button is being pressed its resistance forces increases and so does the fingers pressing force at the moment where a critical pressing depth is reached the buttons resistance force drops largely and immediately in a short period of time the total force on the phone becomes imbalanced causing the phone to shift when we press different buttons different IMU signals will be recorded based on this observation we designed two types of tactile widgets including a family of push buttons and rotary mounts these widgets are complex and passive [Music] it can be a test of phone case and from diverse user interfaces that adapt to specific applications and user preferences [Music] during user interaction the IMU capture strong and unique signals in real-time allowing our algorithm to recognize in real-time what events are produced by which widgets [Music] by carefully designing that there is mechanism we create various resistance force profiles leading to different IMU signals for robust runtime detection this is have many applications even for ",
      start: 13,
      stop: 131,
      source: "Subtitles",
      words: 237,
      sentences: 15.8,
    },
    {
      name: "Demo",
      text:
        "and passive [Music] it can be a test of phone case and from diverse user interfaces that adapt to specific applications and user preferences [Music] during user interaction the IMU capture strong and unique signals in real-time allowing our algorithm to recognize in real-time what events are produced by which widgets [Music] by carefully designing that there is ",
      start: 67,
      stop: 120,
      source: "Subtitles",
      words: 55,
      sentences: 3.6666666666666665,
    },
    {
      name: "Demo",
      text:
        "a simple scenario where the user is wearing a glove in cold weather and cannot touch the screen widgets allow the user to interact with the device without taking off their glove [Music] in another scenario the user tries to adjust the cameras field of view before taking a photo when only a single hand is available the common gesture of pinching on the screen is rather inconvenient digits instead offer a physical interface that allows the camera to be operated in one hand [Music] this user interface can be reconfigured to accommodate different user preferences such as left-handed users [Music] our user event recognition algorithm is robust to accidental motions and environment noise when playing a mobile game the user needs to constantly touch the screen but touching on the screen will also include the screen and cause other inconveniences in contrast widgets interface avoids the occlusion and provides a player game experience similar to a tangible game controller last but not least it enables a virtual saxophone app the vigeous buttons serve as the saxophone keys and the combination of keys allows the user to produce different notes thank you ",
      start: 131,
      stop: 242,
      source: "Subtitles",
      words: 189,
      sentences: 12.6,
    },
    { name: "EndingSlide", text: "", start: 242, stop: 246, source: "Subtitles", words: 0, sentences: 0.0 },
  ],
  DkF8P3qePio: [
    { name: "Introduction", text: "", start: 0, stop: 15, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Demo", text: "", start: 15, stop: 80, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Results", text: "", start: 15, stop: 80, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Method", text: "", start: 80, stop: 188, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Demo", text: "", start: 188, stop: 188, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Results", text: "", start: 188, stop: 197, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "EndingSlide", text: "", start: 197, stop: 209, source: "SpeechToText", words: 0, sentences: 0.0 },
  ],
  E6C0iEZ0M_s: [
    {
      name: "Title",
      text: "[Laughter] ",
      start: 0,
      stop: 8,
      source: "Subtitles",
      words: 1,
      sentences: 0.06666666666666667,
    },
    { name: "Introduction", text: "[Music] ", start: 8, stop: 15, source: "Subtitles", words: 0, sentences: 0.0 },
    {
      name: "SystemArchitecture",
      text: "[Music] ",
      start: 15,
      stop: 35,
      source: "Subtitles",
      words: 0,
      sentences: 0.0,
    },
    { name: "Demo", text: "[Music] ", start: 35, stop: 83, source: "Subtitles", words: 0, sentences: 0.0 },
    { name: "SystemArchitecture", text: "", start: 83, stop: 144, source: "Subtitles", words: 0, sentences: 0.0 },
    {
      name: "EndingSlide",
      text: "you [Music] you ",
      start: 144,
      stop: 169,
      source: "Subtitles",
      words: 2,
      sentences: 0.13333333333333333,
    },
  ],
  HegoKZoKkN8: [
    { name: "Title", text: "", start: 0, stop: 3, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Introduction", text: "", start: 3, stop: 13, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "SystemArchitecture", text: "", start: 13, stop: 13, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Method", text: "", start: 13, stop: 25, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Demo", text: "", start: 25, stop: 49, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Introduction", text: "", start: 26, stop: 28, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "EndingSlide", text: "", start: 49, stop: 54, source: "SpeechToText", words: 0, sentences: 0.0 },
  ],
  KcwjVK8eUdw: [
    { name: "Title", text: "", start: 0, stop: 3, source: "Subtitles", words: 0, sentences: 0.0 },
    {
      name: "Introduction",
      text:
        "understanding how spaces and buildings are used is a key challenge for decision makers in facilities management but understanding occupancy by physical counting is too labor-intensive and installing Hardware sensors would be too expensive previous attempts to visualize indoor occupancy were limited in space and time we engaged with many stakeholders to design implement and deploy ocupado a set of visual decision support tools that use Wi-Fi occupancy data our collaborator sensible building ",
      start: 3,
      stop: 36,
      source: "Subtitles",
      words: 73,
      sentences: 4.866666666666666,
    },
    {
      name: "Method",
      text:
        "science gathers Wi-Fi device data as a proxy for estimating human occupancy we save these location-based counts every five minutes and generate a time series for each zone we did this for hundreds of zones across dozens of buildings we started our design process with a sandbox for idea generation and prototyping after getting a better understanding of stakeholder needs we created the Campus Explorer a feature-rich system that is connected to a live data stream followed by three applications targeted to specific data granularities in both space and time the Campus Explorer has three main ",
      start: 36,
      stop: 83,
      source: "Subtitles",
      words: 96,
      sentences: 6.4,
    },
    {
      name: "Demo",
      text:
        "components the control panel the region selector showing data stripes and the region subset view now showing sparklines users can choose between multiple visualizations depending on the intended task and select independent or absolute lie scales we can click on a region to open the detail view as an overlay a key component is the pinned time series chart activity presets are shortcuts for complex interactions for example to show the regions with the highest device counts in the spatial heat map we can filter regions by building switch to different views like superimposed line charts or use temporal Turing we select the weekend days as discrete time sessions and expect Saturday June 9 in the building long term interface on the Left we can select multiple zones across different floors we can switch between sparklines and average day profiles and click on a zone to open its detailed view if we select a whole floor all of its zones are sized to fit on screen the building recent interface focuses on short term data with glyphs size coded by live device counts in each zone on the Left we see per floor counts from the last half day plus a future prediction or we can switch to see the average activity for a typical day we can also switch the glyphs from live data to the maximum over the past half day or for example to today's average count between 7:00 and 11:00 a.m. the region compared interface lets us choose one specific region in a building and then select a time like June to August and see its time series we pick September to December for our second query and see both results superimposed for a direct comparison across all the charts ",
      start: 83,
      stop: 228,
      source: "Subtitles",
      words: 294,
      sentences: 19.6,
    },
    {
      name: "SystemArchitecture",
      text:
        "components the control panel the region selector showing data stripes and the region subset view now showing sparklines users can choose between multiple visualizations depending on the intended task and select independent or absolute lie scales we can click on a region to open the detail view as an overlay a key component is the pinned time series chart activity presets are shortcuts for complex interactions for example to show the regions with the highest device counts in the spatial heat map we can filter regions by building switch to different views like superimposed line charts or use temporal Turing we select the weekend days as discrete time sessions and expect Saturday June 9 in the building long term interface on the Left we can select multiple zones across different floors we can switch between sparklines and average day profiles and click on a zone to open its detailed view if we select a whole floor all of its zones are sized to fit on screen the building recent interface focuses on short term data with glyphs size coded by live device counts in each zone on the Left we see per floor counts from the last half day plus a future prediction or we can switch to see the average activity for a typical day we can also switch the glyphs from live data to the maximum over the past half day or for example to today's average count between 7:00 and 11:00 a.m. the region compared interface lets us choose one specific region in a building and then select a time like June to August and see its time series we pick September to December for our second query and see both results superimposed for a direct comparison across all the charts ",
      start: 83,
      stop: 228,
      source: "Subtitles",
      words: 294,
      sentences: 19.6,
    },
    {
      name: "EndingSlide",
      text: "you ",
      start: 228,
      stop: 236,
      source: "Subtitles",
      words: 1,
      sentences: 0.06666666666666667,
    },
  ],
  LY6MgDUzS3M: [
    { name: "Title", text: "", start: 0, stop: 8, source: "Subtitles", words: 0, sentences: 0.0 },
    {
      name: "RelatedWork",
      text:
        "capturing a real-world scene for 3d virtual viewing is currently extremely difficult users typically must choose between three strategies that can capture a sparse and unstructured set of images and use an algorithm relying on global mesh estimation these are typically slow and prone to unpredictable failures resulting in a painful trial and error process they can exhaustively sample a dense set of images at the Nyquist rate where the closest scene content only moves one pixel between subsequent images then use light field rendering to generate new views this is feasible for small baseline captures and guarantees high quality renderings but requires thousands to millions of images for wide baseline captures finally they can capture a grid of images below the Nyquist rate and use view interpolation techniques however state-of-the-art methods do not provide a guideline for how low the sampling rate can be for successful reconstruction which again results in a painful trial and error ",
      start: 9,
      stop: 70,
      source: "Subtitles",
      words: 157,
      sentences: 10.466666666666667,
    },
    {
      name: "Introduction",
      text:
        "capturing a real-world scene for 3d virtual viewing is currently extremely difficult users typically must choose between three strategies that can capture a sparse and unstructured set of images and use an algorithm relying on global mesh estimation these are typically slow and prone to unpredictable failures resulting in a painful trial and error process they can exhaustively sample a dense set of images at the Nyquist rate where the closest scene content only moves one pixel between subsequent images then use light field rendering to generate new views this is feasible for small baseline captures and guarantees high quality renderings but requires thousands to millions of images for wide baseline captures finally they can capture a grid of images below the Nyquist rate and use view interpolation techniques however state-of-the-art methods do not provide a guideline for how low the sampling rate can be for successful reconstruction which again results in a painful trial and error ",
      start: 9,
      stop: 70,
      source: "Subtitles",
      words: 157,
      sentences: 10.466666666666667,
    },
    {
      name: "Method",
      text:
        "practical solution for view interpolation from images captured on semi-regular grids we derive a theoretical bound that specifies how densely users need to sample a given scene in practice we apply this bound to capture and re-render real-world scenes and Nyquist view sampling quality while using up to 4,000 times fewer views we train a deep learning pipeline that first promotes each sampled view to an NPI scene representation made up of RGB alpha images sampled evenly in disparity within a camera frustum an NPI can render high-quality continuous views of natural scenes but only within a limited local neighborhood we leverage this to dramatically decrease the required view sampling density by promoting each input view to an MPI we simply render novel views by blending together renderings from adjacent MPs we showcase our methods practicality with viewers that render novel views from our predicted scene representations in real-time on both desktops and mobile phones ",
      start: 71,
      stop: 148,
      source: "Subtitles",
      words: 155,
      sentences: 10.333333333333334,
    },
    {
      name: "Evaluation",
      text:
        "outperforms state-of-the-art view synthesis methods on a diverse variety of challenging scenes we start with the scene of plants in a glass orb containing complex geometry and reflectance effects on the left we see that classic light field interpolation which blends input images reprojected to a constant death plane produces severe ghosting the unstructured luma graph algorithm which depends on a reconstructed global mesh contains jarring artifacts when the mesh contains holes and incorrect geometry such as on the surface of the glass orb the state-of-the-art soft 3d algorithm avoids harsh flickery artifacts by accounting for depth uncertainty but this causes excessive blurring for ambiguous regions such as the semi-transparent glass orb furthermore it aggregates geometry estimation across large neighborhoods which prevents accurate rendering of specularity x' state-of-the-art deep learning based approaches use a convolutional neural network to separately predict geometry and appearance for each rendered view which often results in high frequency artifacts such as those around the orbs rim in this example a reflective pond our method is the only algorithm able to accurately render both the plants floating on the water surface as well as the specular reflection of the background mesh estimation typically fails for large reflections soft 3d reasonably reconstructs diffuse objects but as difficulties with non Lamberson effects on the pond surface deep learning pipelines based on / view geometry prediction displayed jarring flickering artifacts around occlusion edges while our renderings are consistent across the entire camera path in this example of a large fern with complex geometry we see that our method is able to render convincing occlusion effects without the blur found in light field interpolation it is notoriously difficult to estimate meshes where geometry edges align with image edges leading to edge fattening effects in unstructured luma graph renderings soft 3ds renderings contain excessive blur due to the difficulty of using classic stereo depth estimation for repetitive textures such as the fern leaves again we see that backwards warping based deep learning algorithms produce excessive flicker around occlusion edges while our renderings vary smoothly across different viewpoints we demonstrate the robustness of our method by testing on ",
      start: 149,
      stop: 289,
      source: "Subtitles",
      words: 356,
      sentences: 23.733333333333334,
    },
    {
      name: "Demo",
      text:
        "robustness of our method by testing on 60 additional captured scenes these videos were generated with an automated pipeline from input images to rendered novel views without any manual parameter tuning demonstrating the robustness and practicality of our method you ",
      start: 287,
      stop: 333,
      source: "Subtitles",
      words: 40,
      sentences: 2.6666666666666665,
    },
    {
      name: "Results",
      text:
        "robustness of our method by testing on 60 additional captured scenes these videos were generated with an automated pipeline from input images to rendered novel views without any manual parameter tuning demonstrating the robustness and practicality of our method you ",
      start: 287,
      stop: 333,
      source: "Subtitles",
      words: 40,
      sentences: 2.6666666666666665,
    },
    { name: "EndingSlide", text: "", start: 335, stop: 343, source: "Subtitles", words: 0, sentences: 0.0 },
  ],
  LoZQLReSZm0: [
    {
      name: "Title",
      text:
        "hi everyone my name is Ashley sue and I'm a PhD student at Tufts University today I'll be presenting topo lines topological smoothing for line charts the co-authors on this paper with me are Paul Rosen and Christopher Salgado we're both up the University of South Florida and we stopped by the gchat kamilly we like to fall one slides with me you can go to bit dot leet slash topo lines slides so let's get started we use line ",
      start: 0,
      stop: 26,
      source: "Subtitles",
      words: 80,
      sentences: 5.333333333333333,
    },
    {
      name: "Introduction",
      text:
        "charts to visualize many types of data from stock trends to weather changes or from brain activity to radio astronomy frequency but due to the sheer size of data we often find that our line charts can be cluttered or noisy making certain visual analysis tasks difficult for example trying to identify the top three peaks and this EEG data set is particularly hard given the number of data points on our line chart commonly we use line smoothing to help declutter noisy charts smoothing not only helps with noisy data but it also makes performing analysis tasks much easier so I went ahead and applied uniform subsampling to our line chart smoothing the line makes identifying these individual Peaks a lot easier for me here's what I marked as the top three peaks in this EEG data set unfortunately though when I subsampled my data it didn't preserve the original taught three peds from the chart and as a result I didn't get the correct answer that's because the sub sample line didn't retain the most prominent critical points from our input data in fact many popular smoothing techniques unintentionally remove key critical points of being able to identify these points as important to you when smoothing your life chart that meant that you should retain the most prominent peaks from the input data however this is not a guarantee to address the weaknesses of prior approaches we created total lines topo lines is a method for smoothing line charts that leverages techniques from topological data analysis topo lines identifies the most prominent peaks and smooth lines by removing the least important Peaks first top lines also minimizes the residual error between Peaks retaining as much detail from the input data as possible ",
      start: 26,
      stop: 154,
      source: "Subtitles",
      words: 294,
      sentences: 19.6,
    },
    {
      name: "Reflection",
      text:
        "our contributions are as follows we first provide a description for one dimensional topological smoothing we evaluate 5 popular smoothing methods for two low-level tasks across four application domains we show that topo lines is the most effective line smoothing method for many combinations of both data task and data type and finally we demonstrate the general ineffectiveness of several conventional lines with a techniques ",
      start: 154,
      stop: 185,
      source: "Subtitles",
      words: 65,
      sentences: 4.333333333333333,
    },
    {
      name: "Method",
      text:
        "Topa legs requires two steps for topological smoothing the first step is the extraction of the topology of the data which we do by identifying extrema pairs the second is reconstructing the line by removing extrema pairs and smoothing the output by using isotonic regression I'll briefly walk through an example so you can see how Topol aligns preserves the most prominent peaks during smoothing let's begin with the simple line as our input data the first step is identifying the critical points or local minimum or local maxima repair the critical points as extrema pairs using a merge tree I won't get into the mercury calculation for this presentation so please refer to the paper if you'd like those details the persistence is then calculated for each extrema pair and we rank them from lowest to highest persistence pairs with high persistence have important critical points so topo lines preserves the highest persistence peaks during smoothing when we remove pairs for smoothing we have to reconstruct the line topo lines minimizes the residual error by using isotonic regression which is a monotonic regression technique that minimizes the least square error by using isotonic regression we ensure that the original line is modified as little as possible during smoothie so let's recap what we did for our topological smoothing technique using our input data we found the critical pairs and ripped them by their persistence we ensured that topo lines preserves the most prominent Peaks by removing low persistence pairs first during smoothing finally we've reconstructed the line by using isotonic regression to minimize the residual error now that we know how Tocqueville minds works let's move on to our evaluation we compared tilt blinds to ",
      start: 185,
      stop: 303,
      source: "Subtitles",
      words: 286,
      sentences: 19.066666666666666,
    },
    {
      name: "Evaluation",
      text:
        "five popular smoothing methods which you can just see here in this figure we evaluated all these techniques including type the lines on two low level tasks across four application domains the first task is retrieved value which requires finding a function value on a line chart the second task is finding extrema which is identifying the minima and maxima on a line chart as you can see on the most right here took the lines really captures the most prominent peaks from the input data while trying to preserve as much detail from the original line as possible ",
      start: 303,
      stop: 340,
      source: "Subtitles",
      words: 99,
      sentences: 6.6,
    },
    {
      name: "Results",
      text:
        "a summary of our results can be seen on the bottom figure for each application domain we evaluated for the retrieved value task topo lines performed the best across three out of four data sets for the stock data sent took the line to a second best while the medium filter was first the Gaussian smoothing method performed reasonably well overall for this task however uniform cutoff and Douglas pointer all performed consistently poorly for the second task topo lines performed the best or second best on two of the data sets however top lines performed unremarkable for the EEG and stock data sets we believe this is because the high frequency of noise in those data sets makes many of the local extrema that terpenoids is trying to preserve unimportant overall the ductless porker algorithm performed well for this task but uniform cutoff in median performed poorly if either of these analysis tasks are important for your line chart we recommend that those smoothing methods are used with caution ",
      start: 340,
      stop: 410,
      source: "Subtitles",
      words: 170,
      sentences: 11.333333333333334,
    },
    {
      name: "Reflection",
      text:
        "in conclusion we presented a topology based line chart smoothing method we evaluated total lines against five popular smoothing techniques using two low-level tasks across four application domains we also showed that topo lines can perform well for both of these tasks in the future we would like to build an ",
      start: 410,
      stop: 432,
      source: "Subtitles",
      words: 51,
      sentences: 3.4,
    },
    {
      name: "ForwardLooking",
      text:
        "evaluation framework that considers a broader set of tasks and the perceptual differences from various smoothing techniques if you're interested in ",
      start: 432,
      stop: 441,
      source: "Subtitles",
      words: 21,
      sentences: 1.4,
    },
    {
      name: "EndingSlide",
      text:
        "trying to lines out yourself please check out our demo at bitly / purple lines demo the source code is also available with github if you'd like to know more about this work we'd be happy to answer any of your questions thank you for watching you ",
      start: 441,
      stop: 480,
      source: "Subtitles",
      words: 47,
      sentences: 3.1333333333333333,
    },
  ],
  Nym0BJrRUpM: [
    {
      name: "Title",
      text: "we propose a novel method for interactive video completion we are removing the rollerblader from ",
      start: 0,
      stop: 5,
      source: "Subtitles",
      words: 15,
      sentences: 1.0,
    },
    {
      name: "Demo",
      text:
        "the video the user paints a rough mask around the rollerblader our system then removes her from the video and completes the space-time holes the completed video is shown in the right window the rollerblader is still visible the user paints a rough mask again at this frame our system then computes the video completion the user repeats the process through the video you finally we have removed the rollerblader from the video in one minute we are removing the BMX bike from the video the user pains the mask around the bike and our system computes the video completion you for this 80 frame video we painted masks for 15 frames and finished the video completion you ",
      start: 5,
      stop: 108,
      source: "Subtitles",
      words: 118,
      sentences: 7.866666666666666,
    },
    {
      name: "Method",
      text:
        "when computing the color estimation the l1 data term causes temporal visible artifacts as sudden changes of colors on the other hand our l2 data terms successfully removes such artifacts and synthesizes natural-looking results we remove the flamingos in the front the l1 data term causes sudden changes of colors the l2 data term makes the flamingo have double legs to improve the quality we manually modify the frame 40 artifacts are still visible in the result of the l1 dated term on the other hand the l2 dated term successfully improves the results so that the flamingo has single legs we remove a parkour athlete from the video artifacts in the result of a one-day two-term are successfully suppressed in the result of l2 data term is broken by artifacts to fix the fence we manually modified these four frames and reapplied our algorithm the result of l1 dated terms still has artifacts and the result of l2 data term has smoother color transitions we remove the man from the video artifacts in the result of a one-day two-term are successfully suppressed in the result of l2 data term there are artifacts around the legs to remove the artifacts we manually modified these five frames and reapplied our algorithm the result of altitude data term has smoother color transitions the subjective evaluation shows that our ",
      start: 108,
      stop: 245,
      source: "Subtitles",
      words: 226,
      sentences: 15.066666666666666,
    },
    {
      name: "Evaluation",
      text:
        "method produces better result than the state-of-the-art method furthermore our modified version was evaluated as the best ",
      start: 245,
      stop: 255,
      source: "Subtitles",
      words: 17,
      sentences: 1.1333333333333333,
    },
    { name: "EndingSlide", text: "", start: 255, stop: 261, source: "Subtitles", words: 0, sentences: 0.0 },
  ],
  UMFjIRJsBsA: [
    { name: "Title", text: "", start: 0, stop: 7, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Demo", text: "", start: 7, stop: 48, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "SystemArchitecture", text: "", start: 48, stop: 66, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Method", text: "", start: 48, stop: 66, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Demo", text: "", start: 66, stop: 77, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "EndingSlide", text: "", start: 77, stop: 98, source: "SpeechToText", words: 0, sentences: 0.0 },
  ],
  "X-n-Q7c7jN0": [
    { name: "Title", text: "", start: 0, stop: 3, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "SystemArchitecture", text: "", start: 3, stop: 24, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Demo", text: "", start: 24, stop: 62, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "EndingSlide", text: "", start: 62, stop: 66, source: "SpeechToText", words: 0, sentences: 0.0 },
  ],
  "ZKaEM_E-cU8": [
    { name: "Title", text: "", start: 0, stop: 5, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "SystemArchitecture", text: "", start: 5, stop: 68, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Demo", text: "", start: 68, stop: 123, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "EndingSlide", text: "", start: 123, stop: 132, source: "SpeechToText", words: 0, sentences: 0.0 },
  ],
  a3jfyJ9JVeM: [
    { name: "Title", text: "", start: 5, stop: 9, source: "Subtitles", words: 0, sentences: 0.0 },
    {
      name: "VideoOutline",
      text:
        "we built a comprehensive musculoskeletal model and his control system there were produces realistic human movements driven by muscle contraction dynamics the skeleton has 22 joints in the body in a striven by more than 300 muscles activated muscles are shown in red the physically simulator characters are able to react to unexpected perturbations or environmental changes in a physically plausible manner ",
      start: 8,
      stop: 48,
      source: "Subtitles",
      words: 62,
      sentences: 4.133333333333334,
    },
    {
      name: "Method",
      text:
        "our character learned to lift the bar with weights using deep reinforcement learning we gradually increased the ways to learn incrementally more challenging tasks as the mass increases the controller has to learn a different muscle coordination for the increased mass our character also learned to jump from a single reference motion clip which allows us to generate a continuous spectrum of vertical jumps parameterize by target heights as the target height increases with time morph the reference motion to match the increased time in the air the character learned to use arms more dynamically to jump higher ",
      start: 48,
      stop: 102,
      source: "Subtitles",
      words: 98,
      sentences: 6.533333333333333,
    },
    {
      name: "Demo",
      text:
        "you many pathologic gay parents can be attributed to musculoskeletal conditions such as bone deformity and muscle deficiency we consciously created such pathologic conditions in a model the CF gait patterns are generated as intended contracture is the shortening or stiffening of muscles that results in decreased movements and range of motion the contracture of the psoas majors result in permanent flexion of the hip joints the contraction of the gastrocnemius and soleus muscles results in stiff ankles and thus the character is tiptoeing the symptoms of muscle contracture can be easily incorporated into our model by adjusting the rest length of the muscle the contracture of the major muscles in the left thigh results in the stiff knee and consequently a symmetric gait because of the stiff knee our character can not have sufficient foot ground clearance our algorithm learned the control policy of hopping gait that compensates the stiff knee and decreases tripping risk ephemeral anteversion is an inward twisting of the thighbone resulting in in twin gates this character has multiple symptoms including femoral anteversion stiff knees as teeth ankles even with these severe symptoms our algorithm managed to learn in a stable walking controller although the stride is reduced we also learned walking running and dancing with a prosthetic leg the goal of designing a prosthetic leg is restoring the functionality of the missing body parts a learning algorithm simulates the process of adapting to the prosthetic leg this control policy was learned from a normal gait pattern it demonstrates successful running with a prosthetic leg we designed two types of lower extremity prosthesis trustee Beal and trance femoral both have a passive spring damper system to model the compliant reaction of the prosthesis we use the multi-segment foot model when we want to simulate their like a foot motion the multi-segment model has additionally twelve revolute joints and twenty-five muscles over the simplified foot in each leg the sole is represented by linear blackening with respect to foot bones this side-by-side comparison shows the multi segment that absorbs fifth ground impact to make this Moodle ending the difference is subtle yet important in clinical gait analysis the multi segment foot model facilitates more accurate analysis of human gait ",
      start: 102,
      stop: 297,
      source: "Subtitles",
      words: 373,
      sentences: 24.866666666666667,
    },
    {
      name: "RelatedWork",
      text:
        "you predictive surgery simulation begins with preoperative pathologic gait simulates orthopedic surgeries on the musculoskeletal model and aims to generate predictive outcomes of the surgeries we simulated four types of orthopedic surgeries which are performed frequently to cerebral palsy patients femoral d rotational stay atomy is a procedure that corrects rotational deformities in the thigh and helps correct intoing and out join during walking the rectus femoris and hamstrings are large muscles in the front and back of the thigh which have significant influence on walking rectus femoris transfer and distal hamstring lengthening are procedures of transferring a muscle insertion to reduce the muscle tension rectus femoris transfer and distal hamstring lengthening together resulting improving the range of motion in the knee joints pendo achilles lengthening is a procedure the lengthens the achilles tendon to reduce the tension of the calf muscles which can widen the range of motion of the ankle and consequently alleviate the symptoms of tiptoe walking the modified musculoskeletal model ",
      start: 297,
      stop: 371,
      source: "Subtitles",
      words: 164,
      sentences: 10.933333333333334,
    },
    {
      name: "Results",
      text: "learned the post-operative gait that serves as a predictive outcomes of the surgery simulation ",
      start: 371,
      stop: 380,
      source: "Subtitles",
      words: 14,
      sentences: 0.9333333333333333,
    },
    {
      name: "Reflection",
      text:
        "we also have a lot of failure cases in this example one leg squad requires muscle coordination beyond the physical capability of the character which cannot generate sufficiently large pork with flex types and knees designing capable musculoskeletal models in their parameter tuning are important steps for successful simulation and control ",
      start: 380,
      stop: 407,
      source: "Subtitles",
      words: 51,
      sentences: 3.4,
    },
    {
      name: "EndingSlide",
      text: "you ",
      start: 407,
      stop: 426,
      source: "Subtitles",
      words: 1,
      sentences: 0.06666666666666667,
    },
  ],
  aVlfgl32eOk: [{ name: "Demo", text: "", start: 0, stop: 99, source: "SpeechToText", words: 0, sentences: 0.0 }],
  bHLHnv8st0Q: [
    { name: "Title", text: "", start: 0, stop: 2, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Introduction", text: "", start: 1, stop: 21, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Method", text: "", start: 21, stop: 60, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Reflection", text: "", start: 60, stop: 70, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "EndingSlide", text: "", start: 70, stop: 82, source: "SpeechToText", words: 0, sentences: 0.0 },
  ],
  cb1HsKfddYY: [
    { name: "Title", text: "", start: 0, stop: 5, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "SystemArchitecture", text: "", start: 5, stop: 15, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Introduction", text: "", start: 5, stop: 60, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Method", text: "", start: 15, stop: 43, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Results", text: "", start: 43, stop: 60, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Method", text: "", start: 60, stop: 82, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "SystemArchitecture", text: "", start: 60, stop: 82, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Demo", text: "", start: 82, stop: 217, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Reflection", text: "", start: 217, stop: 225, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "EndingSlide", text: "", start: 225, stop: 236, source: "SpeechToText", words: 0, sentences: 0.0 },
  ],
  dDB3xhy55P8: [
    { name: "Title", text: "", start: 0, stop: 5, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Introduction", text: "", start: 5, stop: 29, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Method", text: "", start: 29, stop: 43, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Evaluation", text: "", start: 43, stop: 144, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Results", text: "", start: 144, stop: 219, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "EndingSlide", text: "", start: 219, stop: 224, source: "SpeechToText", words: 0, sentences: 0.0 },
  ],
  faYVdrzq9vU: [
    { name: "Introduction", text: "", start: 0, stop: 30, source: "Subtitles", words: 0, sentences: 0.0 },
    { name: "Title", text: "", start: 30, stop: 47, source: "Subtitles", words: 0, sentences: 0.0 },
    {
      name: "Demo",
      text:
        "and almost invisible cause static moving patterns of sounds invisibly visual when it comes hyperacusis hugs the flow gets in the rhythm of acoustic disruption Danny needs a balance of mind unbalanced auditory system recording stability recalling here it builds up builds up builds up until it overflows divergent sound experiencing inside investigating ",
      start: 47,
      stop: 174,
      source: "Subtitles",
      words: 53,
      sentences: 3.533333333333333,
    },
    {
      name: "EndingSlide",
      text: "you ",
      start: 174,
      stop: 191,
      source: "Subtitles",
      words: 1,
      sentences: 0.06666666666666667,
    },
    { name: "Other", text: "", start: 191, stop: 201, source: "Subtitles", words: 0, sentences: 0.0 },
  ],
  "gciyI-upSjw": [
    { name: "Title", text: "", start: 0, stop: 3, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "SystemArchitecture", text: "", start: 3, stop: 26, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Results", text: "", start: 26, stop: 26, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Demo", text: "", start: 26, stop: 109, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "EndingSlide", text: "", start: 109, stop: 113, source: "SpeechToText", words: 0, sentences: 0.0 },
  ],
  iDn5HXMQNzE: [
    {
      name: "Title",
      text:
        "in this work we present a multiframe super resolution algorithm that supplants demos aching in mobile phone burst photography pipelines our ",
      start: 0,
      stop: 6,
      source: "Subtitles",
      words: 21,
      sentences: 1.4,
    },
    {
      name: "Introduction",
      text:
        "algorithm produces images with higher resolution and improves signal-to-noise ratio it also avoids visual artifacts that are characteristic of demos aching techniques such as chromatic aliasing maze false gradients and more a patterns here we present the steps of how our ",
      start: 6,
      stop: 22,
      source: "Subtitles",
      words: 41,
      sentences: 2.7333333333333334,
    },
    {
      name: "SystemArchitecture",
      text:
        "algorithm merges information from multiple frames the first step of our algorithm is frame acquisition we do not require any special capturing conditions as our method is designed to work with natural hand motion we observed that hand tremor is strong enough to provide displacements of at least multiple pixels within the frame burst when aligned to a common reference frame this translates to random sub-pixel offsets and together with signal aliasing provides sufficient information to reconstruct the higher resolution signal for each captured frame we analyzed a local image gradients localizing edges corners and textured areas this allows us to decide how much resolution enhancement our spatial temporal denoising we wish to obtain based on gradient information we adjust local frame contribution weights we use an isotropic Gaussian radial basis function kernels stretching them along object edges and flattening them in areas where we detect signal variation within expected noise levels as described so far the method would produce strong artifacts in the case of complex non-aligned movement alignment errors or occlusions here we observe that after local alignment the sequence contains many alignment errors without intervention those areas would contribute the fusion artifacts to avoid them we define a local motion robustness model that creates a per frame robustness mask using this we detect misaligned areas and remove their contribution to the final image this allows us to merge such sequences without fusion artifacts now we show the ",
      start: 22,
      stop: 103,
      source: "Subtitles",
      words: 240,
      sentences: 16.0,
    },
    {
      name: "Method",
      text:
        "algorithm merges information from multiple frames the first step of our algorithm is frame acquisition we do not require any special capturing conditions as our method is designed to work with natural hand motion we observed that hand tremor is strong enough to provide displacements of at least multiple pixels within the frame burst when aligned to a common reference frame this translates to random sub-pixel offsets and together with signal aliasing provides sufficient information to reconstruct the higher resolution signal for each captured frame we analyzed a local image gradients localizing edges corners and textured areas this allows us to decide how much resolution enhancement our spatial temporal denoising we wish to obtain based on gradient information we adjust local frame contribution weights we use an isotropic Gaussian radial basis function kernels stretching them along object edges and flattening them in areas where we detect signal variation within expected noise levels as described so far the method would produce strong artifacts in the case of complex non-aligned movement alignment errors or occlusions here we observe that after local alignment the sequence contains many alignment errors without intervention those areas would contribute the fusion artifacts to avoid them we define a local motion robustness model that creates a per frame robustness mask using this we detect misaligned areas and remove their contribution to the final image this allows us to merge such sequences without fusion artifacts now we show the ",
      start: 22,
      stop: 103,
      source: "Subtitles",
      words: 240,
      sentences: 16.0,
    },
    {
      name: "Evaluation",
      text:
        "results of our algorithm compared to other techniques in the first comparison we generated synthetic image bursts from the Kodak and McMaster data sets and compared our algorithm against different demos aching techniques since our algorithm is able to use additional information from multiple frames we obtain a more accurate reconstruction and avoid demos aging artifacts we also compare our algorithm against demos aching techniques using raw bursts captured with a mobile phone we run the demos aching techniques about a single frame as well as the output of the burst merging technique from husana fatale our technique shows more image detail and the least amount of structured artifacts or zippers here we show visual results of our algorithm when used inside the end-to-end burst processing pipeline by has enough at all our our algorithm can merge a varying number of frames in the case of good lighting seven frames is usually enough to provide sufficient sub pixel coverage and additional information to reconstruct a high-quality picture in the case of low light and low signal-to-noise ratio we observe a temporal denoising effect which significantly improves the image quality as the number of merged frames increases for more details we refer you to our ",
      start: 103,
      stop: 197,
      source: "Subtitles",
      words: 204,
      sentences: 13.6,
    },
    {
      name: "Results",
      text:
        "results of our algorithm compared to other techniques in the first comparison we generated synthetic image bursts from the Kodak and McMaster data sets and compared our algorithm against different demos aching techniques since our algorithm is able to use additional information from multiple frames we obtain a more accurate reconstruction and avoid demos aging artifacts we also compare our algorithm against demos aching techniques using raw bursts captured with a mobile phone we run the demos aching techniques about a single frame as well as the output of the burst merging technique from husana fatale our technique shows more image detail and the least amount of structured artifacts or zippers here we show visual results of our algorithm when used inside the end-to-end burst processing pipeline by has enough at all our our algorithm can merge a varying number of frames in the case of good lighting seven frames is usually enough to provide sufficient sub pixel coverage and additional information to reconstruct a high-quality picture in the case of low light and low signal-to-noise ratio we observe a temporal denoising effect which significantly improves the image quality as the number of merged frames increases for more details we refer you to our ",
      start: 103,
      stop: 197,
      source: "Subtitles",
      words: 204,
      sentences: 13.6,
    },
    {
      name: "EndingSlide",
      text: "paper ",
      start: 197,
      stop: 202,
      source: "Subtitles",
      words: 1,
      sentences: 0.06666666666666667,
    },
  ],
  iym8fWxT9QA: [
    { name: "Other", text: "[Music] ", start: 0, stop: 15, source: "Subtitles", words: 0, sentences: 0.0 },
    { name: "Title", text: "", start: 15, stop: 20, source: "Subtitles", words: 0, sentences: 0.0 },
    {
      name: "Introduction",
      text:
        "plants are normally thought of as passive creatures in the environment contrary to this they cannot only sense what's happening around them but also respond and display naturally in our digital and physical interfaces these are the two essential elements to complete an interaction loop through cyber botany we power some of our digital functions with the natural abilities of plants themselves we envision such a bridge between the plants and the digital world to be bi-directional and deeply integrated to explore this we adopt an approach to ",
      start: 20,
      stop: 60,
      source: "Subtitles",
      words: 88,
      sentences: 5.866666666666666,
    },
    {
      name: "SystemArchitecture",
      text:
        "move the electronics from outside to inside the box we keep a plant in a water-soluble polymer product that grows a single conductive channel inside a path we connected this internal wire to sampling instrumentation turning a plant into an antenna or an inconspicuous sensor to detect motion and more in a ",
      start: 61,
      stop: 87,
      source: "Subtitles",
      words: 52,
      sentences: 3.466666666666667,
    },
    {
      name: "Method",
      text:
        "second case study we show examples to not only listen to signals inside the plant but also circuits that could send signals to them we designed a live view software in which clicking on a leave sends it a corresponding signal when a user clicks in the software the corresponding leaf is closed essentially turning the plant into a display integration of sensing and displayed together in this way the by directionality is an important factor for design the pervasiveness of plants then allows us to think of new applications ",
      start: 87,
      stop: 125,
      source: "Subtitles",
      words: 90,
      sentences: 6.0,
    },
    {
      name: "Demo",
      text: "[Music] [Music] [Music] [Music] plants are self-repairing ",
      start: 125,
      stop: 175,
      source: "Subtitles",
      words: 3,
      sentences: 0.2,
    },
    {
      name: "Reflection",
      text:
        "self-regenerating organisms available at scale through cyber botany we envision a convergent design world in which we reappropriation natural capabilities for a new bio interaction design [Music] ",
      start: 175,
      stop: 191,
      source: "Subtitles",
      words: 26,
      sentences: 1.7333333333333334,
    },
    { name: "EndingSlide", text: "[Music] ", start: 191, stop: 211, source: "Subtitles", words: 0, sentences: 0.0 },
  ],
  kopoLzvh5jY: [
    {
      name: "Title",
      text:
        "on earth the simple rules of natural selection and competition led to the evolution of increasingly intelligent ",
      start: 0,
      stop: 6,
      source: "Subtitles",
      words: 17,
      sentences: 1.1333333333333333,
    },
    {
      name: "Introduction",
      text:
        "selection and competition led to the evolution of increasingly intelligent life-forms today we ask if comparably simple rules at multi-agent competition can also lead to intelligent behavior in a new virtual world these agents are playing hide and seek these agents have just begun learning but they've already learned to chase and run away this is a hard world for a hider who has only learned to flee however after training and millions of rounds of hide-and-seek the hiders find a solution ",
      start: 1,
      stop: 31,
      source: "Subtitles",
      words: 82,
      sentences: 5.466666666666667,
    },
    {
      name: "Results",
      text:
        "the hiders learn to use rudimentary tools to their advantage by grabbing and locking these blocks they can create their own shelter the Seekers are locked in place for a brief period at the start of the game giving hiders a chance to prepare even so the hiders must learn to collaborate accomplishing tasks that would be impossible for any single individual the hiders are not the only ones who can learn to use tools after many generations of failing to break into the shelter the Seekers learned to jump over obstacles using ramps however after many millions of rounds of having their shelter breached the hiders learned to take away the primary tool the Seekers have at their disposal note that we did not explicitly incentivize any of these behaviors as each team learns a new skill it implicitly changes the challenges the other team faces creating a new pressure to adapt we've also put these agents into a more open-ended environment randomizing the objects team sizes and walls in this world they learn to construct their own shelter from scratch requiring that they arrange multiple objects into precise structures to prevent seekers from using the ramps the hiders move them to the edge of the play area and lock them in place we originally believe this would be the final strategy that the agents learned however we found that after more training the Seekers discover that they can jump on top of boxes and surf them to the Hydra shelter in the last stage of emergent strategy that we observe the hiders learn to lock as many boxes as they can before constructing their force in order to defend against box surfing so how do ",
      start: 31,
      stop: 130,
      source: "Subtitles",
      words: 287,
      sentences: 19.133333333333333,
    },
    {
      name: "Method",
      text:
        "agents acquire these skills they're trained using reinforcement learning an algorithm inspired by the way animals on earth learn the agents play thousands of rounds of hide-and-seek in parallel for many days they train against each other as well as past versions of themselves using an algorithm called self play coevolution and competition on earth led ",
      start: 130,
      stop: 150,
      source: "Subtitles",
      words: 56,
      sentences: 3.7333333333333334,
    },
    {
      name: "Reflection",
      text:
        "to the only generally intelligent species known to date humans while this world is far less complex than Earth we have found evidence that simple rules can lead to increasingly intelligent behavior from multi-agent interaction we hope that with a much larger and more diverse environment truly complex and intelligent agents will one day emerge ",
      start: 150,
      stop: 173,
      source: "Subtitles",
      words: 55,
      sentences: 3.6666666666666665,
    },
    { name: "EndingSlide", text: "[Music] ", start: 173, stop: 178, source: "Subtitles", words: 0, sentences: 0.0 },
  ],
  "lNri-x2nK7o": [
    {
      name: "Title",
      text:
        "we present CD MPM a set of continuum damage material point methods for efficiently simulating dynamic brittle ",
      start: 0,
      stop: 6,
      source: "Subtitles",
      words: 17,
      sentences: 1.1333333333333333,
    },
    {
      name: "Reflection",
      text:
        "and ductile factors our first method PFF NPM augments the material point method with a variational formulation for crack evolution by incorporating a non local continuum damage based phase filled fracture model through a novel weak form descritization for our second approach we present a new plasticity scheme that supports highly efficient analytic return mapping for a wide variety of plasticity models through this framework we introduce an easy to implement non-associative cam clay model capable of simulating a breath of fracture phenomena although PFF MPM and NACC plasticity work well together in this work we focus on exploring them separately and denote which method was ",
      start: 6,
      stop: 45,
      source: "Subtitles",
      words: 106,
      sentences: 7.066666666666666,
    },
    {
      name: "Introduction",
      text:
        "and ductile factors our first method PFF NPM augments the material point method with a variational formulation for crack evolution by incorporating a non local continuum damage based phase filled fracture model through a novel weak form descritization for our second approach we present a new plasticity scheme that supports highly efficient analytic return mapping for a wide variety of plasticity models through this framework we introduce an easy to implement non-associative cam clay model capable of simulating a breath of fracture phenomena although PFF MPM and NACC plasticity work well together in this work we focus on exploring them separately and denote which method was ",
      start: 6,
      stop: 45,
      source: "Subtitles",
      words: 106,
      sentences: 7.066666666666666,
    },
    {
      name: "Method",
      text:
        "overview of PFF MPA Basics in material space discontinuities approximated with a continuous phase field MDM uses particles to carry the phase field and world space while the phase field evolution equation is discretized on a background scratchpad grid PFF MPM utilizes a staggered integration scheme with two system solves on the grid one for momentum and one for phase the blue and red arrows show key details in our data flow the blue represents the dependence of the grid base saw on particle deformation gradients and the red shows the incorporation of the updated phases in the grid force update we adopt a deviatoric dilation 'el split neo-hookean hyperelasticity to enable elastic degradation within PFF MPM here we show the decomposition of the model into its tensile and compressive components where only the tensile part is allowed to be degraded with three realistic armadillos we compare this hyperelasticity model with the popular fixed CRO rotated and neo-hookean models note our scheme exhibits similar behavior for our second approach to dynamic fracture we propose an easy to implement non-associative can play a plasticity model with a novel fracture friendly hardening scheme please refer to the paper and supplemental document for more detailed explanation compared with the ",
      start: 46,
      stop: 121,
      source: "Subtitles",
      words: 206,
      sentences: 13.733333333333333,
    },
    {
      name: "Evaluation",
      text:
        "cohesive kemplay model used recently by go mat al our NACC model better preserves volume due to our enforcement of a non associative flow rule here we show the effect of changing cohesion beta and hardening alpha for controlling the desired fracture appearance our general return mapping also works with lavon Mises and Drucker Prager yield surfaces here we compare traditional MDM with CDM p.m. for a high velocity impact note that we combine PFF MPM with NACC plasticity to demonstrate their successful union the color reflects the phase field values with red being zero and blue being one the speed of crack propagation can be easily controlled with the crack mobility constant similarly the resistance to damage can be controlled with the energy release rate now we show some more 3d examples ",
      start: 121,
      stop: 173,
      source: "Subtitles",
      words: 133,
      sentences: 8.866666666666667,
    },
    { name: "Results", text: "", start: 173, stop: 173, source: "Subtitles", words: 0, sentences: 0.0 },
    {
      name: "Demo",
      text:
        "we begin by showing some food destruction using NACC plasticity here a sandwich cookie is dropped on the ground next we smash a candy crab and visualize the NIACC hardening parameter in material space at right this multi-material watermelon is thrown at the ground at high speed breaking into many pieces here we're Smashing Pumpkins and this time we visualize the hardening parameter in material space now for some less edible objects using PFF MPM to octocat collide into a bowl with their material space phases visualized during a high-speed bullet impact traditional npm breaks in a physically unrealistic way due to ",
      start: 173,
      stop: 226,
      source: "Subtitles",
      words: 102,
      sentences: 6.8,
    },
    {
      name: "Evaluation",
      text:
        "uncontrollable numerical fracture conversely PFF MPM produces a clear trajectory through the t-rex and here we visualize the base view a combination of von mises plasticity ",
      start: 226,
      stop: 240,
      source: "Subtitles",
      words: 26,
      sentences: 1.7333333333333334,
    },
    { name: "Results", text: "", start: 240, stop: 240, source: "Subtitles", words: 0, sentences: 0.0 },
    {
      name: "Demo",
      text:
        "and PFF MPM phase field damage allows this car smash to produce ductile fractures with intricate shapes here we tear a part of soft armadillo and this time we visualize the face field when twisting a jello bar with robotic arms traditional MPM does not produce any fracture PFF MPM on the other hand produces highly detailed fracture finally to celebrate the introduction of our CDM p.m. methods we like to break some bread with you ",
      start: 240,
      stop: 293,
      source: "Subtitles",
      words: 76,
      sentences: 5.066666666666666,
    },
    { name: "EndingSlide", text: "", start: 293, stop: 301, source: "Subtitles", words: 0, sentences: 0.0 },
  ],
  mBcEYRerITw: [
    {
      name: "Title",
      text: "we present fiocca a visual tool 14 Alexis of algorithms for tomorrow graphic fiber Construction",
      start: 0,
      stop: 7,
      source: "SpeechToText",
      words: 14,
      sentences: 0.9333333333333333,
    },
    {
      name: "Introduction",
      text:
        "I want reinforced parts are widely used for example in the automotive or Aeronautics industry to optimize their beneficial characteristics such as strength and durability the fibers in them need to be characterized very accurately recording for example the length their position and orientation with a computed tomography scan of the fiber-reinforced present their multiple projection images from different angles are required very recent algorithms used to protection images to determine the parameters of the contain fibers reconstruct projection images to a volume and use image analysis methods to determine the fiber parameters",
      start: 7,
      stop: 62,
      source: "SpeechToText",
      words: 92,
      sentences: 6.133333333333334,
    },
    {
      name: "SystemArchitecture",
      text:
        "we design methods for this purpose and implemented them in the field I could hold a list of all loaded results for every result it shows a small preview visualizing a variety of accuracy measures According to which the results can be ranked a histogram visualize its distribution of fiber properties the 3D view provide a large visualization of any chosen without the fibers can be color-coded according to a security measures or Fiber properties the option alliterations that guard informed about the change of accuracy measures and fiber properties provide insight into correlations between accuracy measures and fiber characteristics the interaction protocol and optional settings completed interface",
      start: 62,
      stop: 116,
      source: "SpeechToText",
      words: 107,
      sentences: 7.133333333333334,
    },
    {
      name: "Demo",
      text:
        "let's use fiaca to look at the results if I have a reconstruction algorithm apply to computer tomography scans different resolutions the first four results in the list of them scanned with different resolution sorted by decreasing resolution from top to bottom for the last entry is a man really created reference in the 3D Buda references show with the fibers kalakota to the average match quality we can see two blue fibers fibers that were not identified by any of the results the existence of two such as the similarity measure we can see that these are all quite short however",
      start: 116,
      stop: 194,
      source: "SpeechToText",
      words: 101,
      sentences: 6.733333333333333,
    },
    { name: "EndingSlide", text: "", start: 194, stop: 198, source: "SpeechToText", words: 0, sentences: 0.0 },
  ],
  oUubW4KYBnA: [
    { name: "Title", text: "", start: 0, stop: 8, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Introduction", text: "", start: 8, stop: 30, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Method", text: "", start: 30, stop: 52, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Results", text: "", start: 52, stop: 199, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Evaluation", text: "", start: 72, stop: 173, source: "SpeechToText", words: 0, sentences: 0.0 },
  ],
  u44VpAL2KNU: [
    {
      name: "Introduction",
      text:
        "flat text realist paintings can be lifeless however the world around us is rich with colors textures and patterns we present cats a painting system that synthesizes textures captured from the world in real-time the cat system runs ",
      start: 0,
      stop: 16,
      source: "Subtitles",
      words: 38,
      sentences: 2.533333333333333,
    },
    {
      name: "SystemArchitecture",
      text:
        "on the standard computer with a connected webcam and drawing tablet texture synthesis produces organic results that are not achievable through normal stamping based approaches caps uses an efficient texture synthesis scheme that produces strokes which are non repeating and blend smoothly with each other here in the interface you can ",
      start: 16,
      stop: 45,
      source: "Subtitles",
      words: 51,
      sentences: 3.4,
    },
    {
      name: "Method",
      text:
        "see the exemplar texture in the upper left hand corner in addition to standard painting functions cats is also equipped with features unique to the system for example here the artists is adjusting the brush dryness which changes the speed at which the stroke is committed to the canvas the artists can move the exemplar around until a stroke dries completely if they don't like the results they can always undo the previous actions users can also specify the portion of the exemplar to paint with by cropping to a region of interest jitter is a parameter that affects how much structure from the original texture is maintained the higher the jitter the more the original texture is perturbed exemplar textures can also be saved as swatches for later use a unique aspect of the cat system is a visual coherence between you and the existing strokes the underlying texture synthesis scheme takes the canvas into account when determining which part of the exemplar texture to synthesize from here you can see that when the artist paints the gold parts of the painting they get matched to the gold portion of the exemplar while the fabric gets matched to the fabric texture in the exemplar [Music] [Music] finally users can choose to grab a small portion of the campus to use as the example our texture [Music] [Music] but cats artists can create richly ",
      start: 45,
      stop: 258,
      source: "Subtitles",
      words: 230,
      sentences: 15.333333333333334,
    },
    {
      name: "Demo",
      text:
        "see the exemplar texture in the upper left hand corner in addition to standard painting functions cats is also equipped with features unique to the system for example here the artists is adjusting the brush dryness which changes the speed at which the stroke is committed to the canvas the artists can move the exemplar around until a stroke dries completely if they don't like the results they can always undo the previous actions users can also specify the portion of the exemplar to paint with by cropping to a region of interest jitter is a parameter that affects how much structure from the original texture is maintained the higher the jitter the more the original texture is perturbed exemplar textures can also be saved as swatches for later use a unique aspect of the cat system is a visual coherence between you and the existing strokes the underlying texture synthesis scheme takes the canvas into account when determining which part of the exemplar texture to synthesize from here you can see that when the artist paints the gold parts of the painting they get matched to the gold portion of the exemplar while the fabric gets matched to the fabric texture in the exemplar [Music] [Music] finally users can choose to grab a small portion of the campus to use as the example our texture [Music] [Music] but cats artists can create richly ",
      start: 45,
      stop: 258,
      source: "Subtitles",
      words: 230,
      sentences: 15.333333333333334,
    },
    {
      name: "Results",
      text:
        "textured paintings not easily achievable through purely digital or traditional means see our paper for details ",
      start: 258,
      stop: 273,
      source: "Subtitles",
      words: 16,
      sentences: 1.0666666666666667,
    },
  ],
  ucn2lZq5RMA: [
    {
      name: "Introduction",
      text:
        "earthquakes caused major destruction of life and property all around the world they mostly occur at tectonic plate boundaries Southern California has a large number of active seismic faults seismologists are performing detailed earthquake simulation in this region to better understand earthquakes and minimize their hazard we present a ",
      start: 0,
      stop: 22,
      source: "Subtitles",
      words: 49,
      sentences: 3.2666666666666666,
    },
    {
      name: "Method",
      text:
        "visualization of an earthquake simulation for a region 600 kilometers long 300 kilometers wide and 80 kilometers deep the simulation ran on 240 processors for 4 days on a supercomputer and yielded 10 terabytes of data visualization helps the ",
      start: 22,
      stop: 42,
      source: "Subtitles",
      words: 39,
      sentences: 2.6,
    },
    {
      name: "Demo",
      text:
        "scientists to quickly analyze this huge stockpile of data this animation shows the velocity of wave propagation through the region in three dimensions and highlights its complex nature the surface velocity is displayed as topographic deformation of the map on the bottom the earthquake originates ",
      start: 42,
      stop: 69,
      source: "Subtitles",
      words: 45,
      sentences: 3.0,
    },
    {
      name: "Results",
      text:
        "about 60 miles south of Palm Springs near the Salton Sea the rupture travels Northwest along the fault this animation shows the expected ground motions produced by a 7.7 quake on the southern San Andreas Fault as time elapses the ground motion from the seismic wave is visible as exaggerated topographic deformation even though the rupture event on the fault lasts little over a minute ground shaking continues for more than three minutes in a widespread area in this animation the fault slip rate is displayed along the 16 kilometer deep vertical plane the surface wave speed is shown along the horizontal plane on both sides of the fault plane in this simulation one can see how an earthquake effects different regions impacting millions of Southern Californians potentially resulting in significant loss of human life and billions of dollars in damage ",
      start: 69,
      stop: 170,
      source: "Subtitles",
      words: 141,
      sentences: 9.4,
    },
    { name: "EndingSlide", text: "", start: 170, stop: 177, source: "Subtitles", words: 0, sentences: 0.0 },
  ],
  wNu137Y6Vb0: [
    { name: "Title", text: "", start: 0, stop: 4, source: "Subtitles", words: 0, sentences: 0.0 },
    { name: "Reflection", text: "", start: 4, stop: 17, source: "Subtitles", words: 0, sentences: 0.0 },
    {
      name: "SystemArchitecture",
      text: "[Music] [Music] [Music] ",
      start: 17,
      stop: 62,
      source: "Subtitles",
      words: 0,
      sentences: 0.0,
    },
    {
      name: "Demo",
      text:
        "[Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] what if you could invent a new resource every day ",
      start: 17,
      stop: 176,
      source: "Subtitles",
      words: 10,
      sentences: 0.6666666666666666,
    },
    {
      name: "EndingSlide",
      text: "you ",
      start: 176,
      stop: 181,
      source: "Subtitles",
      words: 1,
      sentences: 0.06666666666666667,
    },
  ],
  x4O8pojMF0w: [
    {
      name: "Introduction",
      text:
        "we tried to build robots that learn a little bit like humans do by trial and error what we've done is trained an algorithm to solve the Rubik's Cube one-handed with a robotic captain which is actually pretty hard even for a human to do we don't tell it how the hand is to move the the cube in order to get there the particular friction that's on the fingers how easy it is to turn the faces on the cube what the gravity what the weight of the cube is all of these things it needs to learn by itself the interesting thing is that kind of standard techniques in robotics haven't been able to scale to that complexity that we see in a robotic hand humans have evolved to be able to manipulate and operate our hands so there's a huge amount of learning that's happened through evolution to get us to this point as a as a species and the robot has to learn all of this from scratch ",
      start: 0,
      stop: 70,
      source: "Subtitles",
      words: 172,
      sentences: 11.466666666666667,
    },
    {
      name: "Method",
      text:
        "instead of trying to write very dedicated algorithms to operate such a hand we took a different approach where we create thousands of different simulated environments and learn to do the task in all of those and hopefully the robotic hand will be able to do it in the real world as well this means like thousands of years of experience that is your network has had in simulation every time the argument good at the task we make the task harder that's really crucial because you need exposure to really complicate environments in order to eventually be robust to the real world you put a rubber glove on their hand and can still carry out the task this ability to generalize to new ",
      start: 70,
      stop: 120,
      source: "Subtitles",
      words: 124,
      sentences: 8.266666666666667,
    },
    {
      name: "Reflection",
      text:
        "environments feels like a very poor piece of intelligence it really changes the way we think about training of general purpose robots moving from thinking too much about the actual arguments and start thinking about how do we create complex enough worlds where they can learn at some point then it would be more down to the imagination what robots could actually accomplish they hope is to build robots that can do many different tasks to increase the standard of living and give everybody a better life [Music] ",
      start: 120,
      stop: 165,
      source: "Subtitles",
      words: 87,
      sentences: 5.8,
    },
    { name: "EndingSlide", text: "", start: 165, stop: 170, source: "Subtitles", words: 0, sentences: 0.0 },
  ],
  ypVWlWcR7Qk: [
    { name: "Title", text: "", start: 0, stop: 3, source: "Subtitles", words: 0, sentences: 0.0 },
    {
      name: "Introduction",
      text:
        "Online ads are often disruptively competing for the user's attention. This has detrimental effects on the user's\nbrowsing experience. Therefore, more and more users install ad-blockers. As most commercial web services are primarily monetized through advertisements, this development provides a serious threat. To counter this, we explored gamified ads, that can be playfully deactivated. We investigated whether those enhance user\nenjoyment and ad effectiveness. We created eight game concepts that differ\nin their degree of interactivity, and whether the affect the entire website\nor just the ad. In the course of a study with 20 participants we made sure that the storyboards explain ",
      start: 3,
      stop: 47,
      source: "Subtitles",
      words: 99,
      sentences: 6.6,
    },
    {
      name: "Method",
      text:
        "the intended concepts. Next we evaluated the acceptance and the perception of the game concepts using the storyboards in another study with 50 participants. We implemented three well-perceived concepts and integrated them into an exemplary newspage. This newspage contains various categorized articles and fictitious advertisements for playful\ndeactivation. In the first Concept, a game of Tetris is\nplayed to deactivate the ad. As soon as a row is cleared, the advertisement disappears. In the second Concept, the ad has to be covered by Color splashes. Once the ad is completely covered\nit disappears as well. In the third concept, a small monster needs to be dragged onto the ad. This monster then eats the advertisement and receives an upgrade. For the baseline without playful deactivation, advertisements are deactivated by clicking a button in the upper right corner. In a lab study with 72 participants, we found that the gamified concepts are more ",
      start: 47,
      stop: 119,
      source: "Subtitles",
      words: 149,
      sentences: 9.933333333333334,
    },
    {
      name: "Evaluation",
      text:
        "enjoyable than deactivating advertisements without game elements. The monster concept was even preferred over using the adblocker. Also, playfully deactivating advertisements had a positive impact on users\u2019 brand- and product memory, enhancing the advertising effectiveness. This indicates that our approach is a promising way of bridging the gap between user enjoyment\nand effective advertising. ",
      start: 119,
      stop: 150,
      source: "Subtitles",
      words: 53,
      sentences: 3.533333333333333,
    },
    { name: "EndingSlide", text: "", start: 150, stop: 157, source: "Subtitles", words: 0, sentences: 0.0 },
  ],
  yxhGWds_g4I: [
    { name: "Title", text: "", start: 0, stop: 2, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Introduction", text: "", start: 2, stop: 37, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Method", text: "", start: 2, stop: 37, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Evaluation", text: "", start: 37, stop: 60, source: "SpeechToText", words: 0, sentences: 0.0 },
    { name: "Results", text: "", start: 60, stop: 308, source: "SpeechToText", words: 0, sentences: 0.0 },
  ],
};
